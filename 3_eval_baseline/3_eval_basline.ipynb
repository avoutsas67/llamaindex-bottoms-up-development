{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Bottoms-Up Development - Evaluation Baseline\n",
    "\n",
    "LlamaIndex provides some basic evaluation of query engines! We can setup an evaluator that will measure both hallucinations, as well as if the query was actually answered!\n",
    "\n",
    "This is provided by two main evaluations:\n",
    "\n",
    "- `ResponseSourceEvaluator` - uses an LLM to decide if the response is similar enough to the sources -- a good measure for hallunication detection!\n",
    "- `QueryResponseEvaluator` - uses an LLM to decide if a response is similar enough to the original query -- a good measure for checking if the query was answered!\n",
    "\n",
    "You may have noticed that we are using an LLM for this task. That means we will want to pick a powerful LLM, like GPT-4 or Claude-2.\n",
    "\n",
    "Lastly, using these methods, we can also use the LLM to generate syntheic questions to evaluate with!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Baseline Query Engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "OpenAI version: 1.33.0\n",
      "llamaindex version: 0.10.43\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv  # type: ignore\n",
    "\n",
    "# ## Using the OpenAI LLM with the VectorStoreIndex\n",
    "from openai import __version__ as openai_version  # type: ignore\n",
    "from llama_index.core import __version__ as llama_index_version  # type: ignore\n",
    "\n",
    "# Load environment variables\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"OpenAI version: {openai_version}\")\n",
    "print(f\"llamaindex version: {llama_index_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "OpenAI version: 1.33.0\n",
      "llamaindex version: 0.10.43\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv  # type: ignore\n",
    "\n",
    "# ## Using the OpenAI LLM with the VectorStoreIndex\n",
    "from openai import __version__ as openai_version  # type: ignore\n",
    "from llama_index.core import __version__ as llama_index_version  # type: ignore\n",
    "\n",
    "# Load environment variables\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"OpenAI version: {openai_version}\")\n",
    "print(f\"llamaindex version: {llama_index_version}\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import MarkdownReader\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "\n",
    "def load_markdown_docs(filepath):\n",
    "    \"\"\"Load markdown docs from a directory, excluding all other file types.\"\"\"\n",
    "    loader = SimpleDirectoryReader(\n",
    "        input_dir=filepath,\n",
    "        required_exts=[\".md\"],\n",
    "        file_extractor={\".md\": MarkdownReader()},\n",
    "        recursive=True,\n",
    "    )\n",
    "\n",
    "    documents = loader.load_data()\n",
    "\n",
    "    # exclude some metadata from the LLM\n",
    "    for doc in documents:\n",
    "        doc.excluded_llm_metadata_keys = [\"File Name\", \"Content Type\", \"Header Path\"]\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our documents from each folder.\n",
    "# we keep them seperate for now, in order to create seperate indexes later\n",
    "getting_started_docs = load_markdown_docs(\"../docs/getting_started\")\n",
    "community_docs = load_markdown_docs(\"../docs/community\")\n",
    "data_docs = load_markdown_docs(\"../docs/core_modules/data_modules\")\n",
    "agent_docs = load_markdown_docs(\"../docs/core_modules/agent_modules\")\n",
    "model_docs = load_markdown_docs(\"../docs/core_modules/model_modules\")\n",
    "query_docs = load_markdown_docs(\"../docs/core_modules/query_modules\")\n",
    "supporting_docs = load_markdown_docs(\"../docs/core_modules/supporting_modules\")\n",
    "tutorials_docs = load_markdown_docs(\"../docs/end_to_end_tutorials\")\n",
    "contributing_docs = load_markdown_docs(\"../docs/development\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# create a global service context\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "\n",
    "# create a vector store index for each folder\n",
    "try:\n",
    "    getting_started_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./getting_started_index\")\n",
    "    )\n",
    "    community_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./community_index\")\n",
    "    )\n",
    "    data_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./data_index\")\n",
    "    )\n",
    "    agent_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./agent_index\")\n",
    "    )\n",
    "    model_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./model_index\")\n",
    "    )\n",
    "    query_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./query_index\")\n",
    "    )\n",
    "    supporting_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./supporting_index\")\n",
    "    )\n",
    "    tutorials_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./tutorials_index\")\n",
    "    )\n",
    "    contributing_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./contributing_index\")\n",
    "    )\n",
    "except:\n",
    "    getting_started_index = VectorStoreIndex.from_documents(getting_started_docs)\n",
    "    getting_started_index.storage_context.persist(persist_dir=\"./getting_started_index\")\n",
    "\n",
    "    community_index = VectorStoreIndex.from_documents(community_docs)\n",
    "    community_index.storage_context.persist(persist_dir=\"./community_index\")\n",
    "\n",
    "    data_index = VectorStoreIndex.from_documents(data_docs)\n",
    "    data_index.storage_context.persist(persist_dir=\"./data_index\")\n",
    "\n",
    "    agent_index = VectorStoreIndex.from_documents(agent_docs)\n",
    "    agent_index.storage_context.persist(persist_dir=\"./agent_index\")\n",
    "\n",
    "    model_index = VectorStoreIndex.from_documents(model_docs)\n",
    "    model_index.storage_context.persist(persist_dir=\"./model_index\")\n",
    "\n",
    "    query_index = VectorStoreIndex.from_documents(query_docs)\n",
    "    query_index.storage_context.persist(persist_dir=\"./query_index\")\n",
    "\n",
    "    supporting_index = VectorStoreIndex.from_documents(supporting_docs)\n",
    "    supporting_index.storage_context.persist(persist_dir=\"./supporting_index\")\n",
    "\n",
    "    tutorials_index = VectorStoreIndex.from_documents(tutorials_docs)\n",
    "    tutorials_index.storage_context.persist(persist_dir=\"./tutorials_index\")\n",
    "\n",
    "    contributing_index = VectorStoreIndex.from_documents(contributing_docs)\n",
    "    contributing_index.storage_context.persist(persist_dir=\"./contributing_index\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Query Engine Tools\n",
    "\n",
    "Since we have so many indicies, we can create a query engine tool for each and then use them in a single query engine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# create a query engine tool for each folder\n",
    "getting_started_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=getting_started_index.as_query_engine(),\n",
    "    name=\"Getting Started\",\n",
    "    description=\"Useful for answering questions about installing and running llama index, as well as basic explanations of how llama index works.\",\n",
    ")\n",
    "\n",
    "community_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=community_index.as_query_engine(),\n",
    "    name=\"Community\",\n",
    "    description=\"Useful for answering questions about integrations and other apps built by the community.\",\n",
    ")\n",
    "\n",
    "data_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=data_index.as_query_engine(),\n",
    "    name=\"Data Modules\",\n",
    "    description=\"Useful for answering questions about data loaders, documents, nodes, and index structures.\",\n",
    ")\n",
    "\n",
    "agent_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=agent_index.as_query_engine(),\n",
    "    name=\"Agent Modules\",\n",
    "    description=\"Useful for answering questions about data agents, agent configurations, and tools.\",\n",
    ")\n",
    "\n",
    "model_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=model_index.as_query_engine(),\n",
    "    name=\"Model Modules\",\n",
    "    description=\"Useful for answering questions about using and configuring LLMs, embedding modles, and prompts.\",\n",
    ")\n",
    "\n",
    "query_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_index.as_query_engine(),\n",
    "    name=\"Query Modules\",\n",
    "    description=\"Useful for answering questions about query engines, query configurations, and using various parts of the query engine pipeline.\",\n",
    ")\n",
    "\n",
    "supporting_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=supporting_index.as_query_engine(),\n",
    "    name=\"Supporting Modules\",\n",
    "    description=\"Useful for answering questions about supporting modules, such as callbacks, service context, and avaluation.\",\n",
    ")\n",
    "\n",
    "tutorials_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=tutorials_index.as_query_engine(),\n",
    "    name=\"Tutorials\",\n",
    "    description=\"Useful for answering questions about end-to-end tutorials and giving examples of specific use-cases.\",\n",
    ")\n",
    "\n",
    "contributing_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=contributing_index.as_query_engine(),\n",
    "    name=\"Contributing\",\n",
    "    description=\"Useful for answering questions about contributing to llama index, including how to contribute to the codebase and how to build documentation.\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Unified Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for notebooks\n",
    "import nest_asyncio\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=[\n",
    "        getting_started_tool,\n",
    "        community_tool,\n",
    "        data_tool,\n",
    "        agent_tool,\n",
    "        model_tool,\n",
    "        query_tool,\n",
    "        supporting_tool,\n",
    "        tutorials_tool,\n",
    "        contributing_tool,\n",
    "    ],\n",
    "    # enable this for streaming\n",
    "    # response_synthesizer=get_response_synthesizer(streaming=True),\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Query Engine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install llama index, you can follow these steps:\n",
      "1. For installation from Pip, simply run `pip install llama-index`.\n",
      "2. For installation from Source, clone the repository using `git clone https://github.com/jerryjliu/llama_index.git`, then either:\n",
      "   - Run `pip install -e .` for an editable install of just the package itself.\n",
      "   - Run `pip install -r requirements.txt` to install optional dependencies and dependencies used for development.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do I install llama index?\")\n",
    "print(str(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Baseline!\n",
    "\n",
    "Now that we have our baseline query engine created, we can create a basic evaluation pipeline!\n",
    "\n",
    "Our pipeline will:\n",
    "\n",
    "- Generate a small dataset of questions\n",
    "- Save/cache these questions (so we can properly compare performance later!)\n",
    "- Evaluate both response quality and hallucination\n",
    "\n",
    "To do this reliably, we need to use an LLM smarter than `gpt-3.5-turbo`, so we will setup `gpt-4` for the evaluation process!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Dataset\n",
    "\n",
    "In order to make the question generation more effecient, we can remove small documents and combine all documents into a giant single docoument.\n",
    "\n",
    "I also modify the question generation prompt, to generate a single question for each chunk, along with extra context for what it is reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"../docs\", recursive=True, required_exts=[\".md\"]\n",
    ").load_data()\n",
    "\n",
    "all_text = \"\"\n",
    "\n",
    "for doc in documents:\n",
    "    all_text += doc.text\n",
    "\n",
    "giant_document = Document(text=all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.evaluation import DatasetGenerator\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "llm_gpt4 = OpenAI(llm=\"gpt-4\", temperature=0)\n",
    "\n",
    "question_dataset = []\n",
    "if os.path.exists(\"question_dataset.txt\"):\n",
    "    with open(\"question_dataset.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            question_dataset.append(line.strip())\n",
    "else:\n",
    "    # generate questions\n",
    "    data_generator = DatasetGenerator.from_documents(\n",
    "        [giant_document],\n",
    "        text_question_template=PromptTemplate(\n",
    "            \"A sample from the LlamaIndex documentation is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Using the documentation sample, carefully follow the instructions below:\\n\"\n",
    "            \"{query_str}\"\n",
    "        ),\n",
    "        question_gen_query=(\n",
    "            \"You are an evaluator for a search pipeline. Your task is to write a single question \"\n",
    "            \"using the provided documentation sample above to test the search pipeline. The question should \"\n",
    "            \"reference specific names, functions, and terms. Restrict the question to the \"\n",
    "            \"context information provided.\\n\"\n",
    "            \"Question: \"\n",
    "        ),\n",
    "    )\n",
    "    generated_questions = data_generator.generate_questions_from_nodes()\n",
    "\n",
    "    # randomly pick 40 questions from each dataset\n",
    "    generated_questions = random.sample(generated_questions, 40)\n",
    "    question_dataset.extend(generated_questions)\n",
    "\n",
    "    print(f\"Generated {len(question_dataset)} questions.\")\n",
    "\n",
    "    # save the questions!\n",
    "    with open(\"question_dataset.txt\", \"w\") as f:\n",
    "        for question in question_dataset:\n",
    "            f.write(f\"{question.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the function used to specify the metadata visible to the embedding model and how can it be customized?', 'How can I convert tools to LangChain tools using the provided documentation sample?', 'What is the purpose of the \"router query engine\" in the LlamaIndex framework?', 'What are the different vector stores supported by LlamaIndex for use as the storage backend for `VectorStoreIndex`?', 'What is the default number of LLM calls required for the ListIndex?']\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(question_dataset, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate with the Dataset\n",
    "\n",
    "Now that we have our dataset, let's measure performance!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Response for Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "from llama_index.core import Response\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "def evaluate_query_engine(evaluator, query_engine, questions):\n",
    "    async def run_query(query_engine, q):\n",
    "        try:\n",
    "            return await query_engine.query(q)\n",
    "        except:\n",
    "            return Response(response=\"Error, query failed.\")\n",
    "\n",
    "    total_correct = 0\n",
    "    all_results = []\n",
    "    for batch_size in range(0, len(questions), 5):\n",
    "        batch_qs = questions[batch_size : batch_size + 5]\n",
    "\n",
    "        tasks = [run_query(query_engine, q) for q in batch_qs]\n",
    "        responses = asyncio.run(asyncio.gather(*tasks))\n",
    "        print(f\"finished batch {(batch_size // 5) + 1} out of {len(questions) // 5}\")\n",
    "\n",
    "        for response in responses:\n",
    "            if evaluator.evaluate_response(response=response).passing:\n",
    "                eval_result = 1\n",
    "            else:\n",
    "                eval_result = 0\n",
    "            total_correct += eval_result\n",
    "            all_results.append(eval_result)\n",
    "\n",
    "        # helps avoid rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    return total_correct, all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished batch 1 out of 8\n",
      "finished batch 2 out of 8\n",
      "finished batch 3 out of 8\n",
      "finished batch 4 out of 8\n",
      "finished batch 5 out of 8\n",
      "finished batch 6 out of 8\n",
      "finished batch 7 out of 8\n",
      "finished batch 8 out of 8\n",
      "Hallucination? Scored 0 out of 40 questions correctly.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "# gpt-4 evaluator!\n",
    "evaluator = FaithfulnessEvaluator(llm=llm_gpt4)\n",
    "\n",
    "total_correct, all_results = evaluate_query_engine(\n",
    "    evaluator, query_engine, question_dataset\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Hallucination? Scored {total_correct} out of {len(question_dataset)} questions correctly.\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What are the possible settings for the LLM and how can the user set the prompt for term extraction?'\n",
      " 'How can I convert tools to LangChain tools using the provided documentation sample?'\n",
      " 'What is the purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the SubQuestionQueryEngine class in LlamaIndex?'\n",
      " 'What is the purpose of the `query_wrapper_prompt` in the `HuggingFaceLLM` class?'\n",
      " 'What embedding model does LlamaIndex use by default?'\n",
      " 'What are the available options for the storage backend of the index store in LlamaIndex?'\n",
      " 'What is the function used to specify the metadata visible to the embedding model and how can it be customized?'\n",
      " 'What are the node postprocessors available in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the DEFAULT_REFINE_PROMPT_SEL_LC in the LlamaIndex documentation?'\n",
      " \"What is the purpose of the `CollectionQueryConsumer` class in the Delphic application's WebSocket handling?\"\n",
      " 'What is the purpose of the `ReActAgent` and how can it be initialized with other agents as tools?'\n",
      " 'How can I create a Django superuser using the Delphic application?'\n",
      " 'What is the default number of LLM calls required for the ListIndex?'\n",
      " 'What are the different vector stores supported by LlamaIndex for use as the storage backend for `VectorStoreIndex`?'\n",
      " 'What are the key building blocks in LlamaIndex for composing a Retrieval Augmented Generation (RAG) pipeline?'\n",
      " 'What is the purpose of the \"router query engine\" in the LlamaIndex framework?'\n",
      " 'What is the purpose of the `VectorStoreIndex` class in the LlamaIndex documentation sample?'\n",
      " 'What storage backends are supported by LlamaIndex for persisting data?'\n",
      " 'What is the purpose of the `fetchDocuments` function in the `fetchDocuments.tsx` file in the React frontend?'\n",
      " 'What is the purpose of the `DecomposeQueryTransform` module in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the `RefinePrompt` class in the LlamaIndex documentation?'\n",
      " \"What is the function used to retrieve the collections for the logged-in user in the Delphic project's frontend?\"\n",
      " 'What is the default value for the `include_metadata` parameter in the `SimpleNodeParser` class?'\n",
      " 'What is the purpose of the `RouterQueryEngine` in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the TokenCountingHandler callback in the LlamaIndex library?'\n",
      " 'What is the purpose of the `RelatedNodeInfo` class in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the ResponseEvaluator class in the LlamaIndex library?'\n",
      " 'What is the purpose of the `SimilarityPostprocessor` in the query engine configuration?'\n",
      " 'What is the purpose of the Algovera tool built on top of LlamaIndex?'\n",
      " 'How can you delete a document from the index data structures and specify whether to delete it from the document store as well?'\n",
      " 'How can we extract terms from documents using LlamaIndex and insert them into the index?'\n",
      " 'What is the purpose of the HyDE query transform in the LlamaIndex?'\n",
      " 'What are the three primary sections within the layout of the ChatView component?'\n",
      " 'What is the default value for the child_branch_factor parameter when configuring the ComposableGraphQueryEngine?'\n",
      " 'What is the purpose of the `insert_terms` function in the LlamaIndex app?'\n",
      " 'What is the purpose of the `load_collection_model` function in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the SQLTableNodeMapping object in the LlamaIndex documentation sample?'\n",
      " 'What is the purpose of the `RouterQueryEngine` in LlamaIndex and how can it be used in the search pipeline?']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hallucinated_questions = np.array(question_dataset)[np.array(all_results) == 0]\n",
    "print(hallucinated_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation is to provide guidance on customizing prompt sizes for different LLMs, helping users optimize performance based on the specific context length of each model.\n",
      "-----------------\n",
      "> Source (Doc id: 02dc8324-04fd-43e7-abd4-f85af1ee2cc3): Sub question: What is the purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation?\n",
      "Response: The purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation is to provide guidance on how to customize prompt sizes for different LLMs. This class helps users adjust the internal prompts to optimize performance based on the specific context length of each model.\n",
      "\n",
      "> Source (Doc id: d89f3f5b-6963-47fb-9a0e-480603715b1a): Sub question: How is the `GuidancePydanticProgram` class used in LlamaIndex?\n",
      "Response: To use the `GuidancePydanticProgram` class in LlamaIndex, you can follow the method described in the context. By implementing the class and ensuring that the generated tokens are returned, the class should work effectively within the LlamaIndex framework. It's important to note that adjusting the internal prompts may be necessary for optimal performance, and using a sufficiently large LLM is recommended to handle the complex queries that LlamaIndex processes. Additionally, customizing the prompt sizes using the prompt helper is advised to accommodate different context lengths specific to each model.\n",
      "\n",
      "> Source (Doc id: e1946473-d90d-43bf-afd0-0dce34b37e11): Sub question: Can the `GuidancePydanticProgram` class be customized or extended?\n",
      "Response: Yes, the `GuidancePydanticProgram` class can be customized or extended by creating a subclass that inherits from it. This allows for adding new functionality or modifying existing behavior while leveraging the features provided by the base class.\n",
      "\n",
      "> Source (Doc id: cf890995-b3fa-4253-90fe-560c52edede8): Concept\n",
      "\n",
      "Prompting is the fundamental input that gives LLMs their expressive power. LlamaIndex uses prompts to build the index, do insertion, \n",
      "perform traversal during querying, and to synthesize the final answer.\n",
      "\n",
      "LlamaIndex uses a set of default prompt templates that work well out of the box.\n",
      "\n",
      "In addition, there are some prompts written and used specifically for chat models like `gpt-3.5-turbo` here.\n",
      "\n",
      "Users may also provide their own prompt templates to further customize the behavior of the framework. The best method for customizing is copying the default prompt from the link above, and using that as the base for any modifications.\n",
      "\n",
      "> Source (Doc id: fc96fcf5-efbe-43e8-8e35-331c326cf4a4): Query and print response\n",
      "query_engine = index.as_query_engine()\n",
      "response = query_engine.query(\"\")\n",
      "print(response)\n",
      "```\n",
      "\n",
      "Using this method, you can use any LLM. Maybe you have one running locally, or running on your own server. As long as the class is implemented and the generated tokens are returned, it should work out. Note that we need to use the prompt helper to customize the prompt sizes, since every model has a slightly different context length.\n",
      "\n",
      "Note that you may have to adjust the internal prompts to get good performance. Even then, you should be using a sufficiently large LLM to ensure it's capable of handling the complex queries that LlamaIndex uses internally, so your mileage may vary.\n",
      "\n",
      "A list of all default internal prompts is available here, and chat-specific prompts are listed here. You can also implement your own custom prompts, as described here.\n",
      "\n",
      "> Source (Doc id: fc96fcf5-efbe-43e8-8e35-331c326cf4a4): Query and print response\n",
      "query_engine = index.as_query_engine()\n",
      "response = query_engine.query(\"\")\n",
      "print(response)\n",
      "```\n",
      "\n",
      "Using this method, you can use any LLM. Maybe you have one running locally, or running on your own server. As long as the class is implemented and the generated tokens are returned, it should work out. Note that we need to use the prompt helper to customize the prompt sizes, since every model has a slightly different context length.\n",
      "\n",
      "Note that you may have to adjust the internal prompts to get good performance. Even then, you should be using a sufficiently large LLM to ensure it's capable of handling the complex queries that LlamaIndex uses internally, so your mileage may vary.\n",
      "\n",
      "A list of all default internal prompts is available here, and chat-specific prompts are listed here. You can also implement your own custom prompts, as described here.\n",
      "\n",
      "> Source (Doc id: 0153d984-4bbc-4fc7-8730-877ceeac75d0): Example: Explicitly configure `context_window` and `num_output`\n",
      "\n",
      "If you are using other LLM classes from langchain, you may need to explicitly configure the `context_window` and `num_output` via the `ServiceContext` since the information is not available by default.\n",
      "\n",
      "```python\n",
      "\n",
      "from llama_index import (\n",
      "    KeywordTableIndex,\n",
      "    SimpleDirectoryReader,\n",
      "    ServiceContext\n",
      ")\n",
      "from llama_index.llms import OpenAI\n",
      "\n",
      "> Source (Doc id: 2853d29c-0b41-422e-8a11-3e7a3b80e71a): Custom Embedding Model\n",
      "\n",
      "If you wanted to use embeddings not offered by LlamaIndex or Langchain, you can also extend our base embeddings class and implement your own!\n",
      "\n",
      "The example below uses Instructor Embeddings (install/setup details here), and implements a custom embeddings class. Instructor embeddings work by providing text, as well as \"instructions\" on the domain of the text to embed. This is helpful when embedding text from a very specific and specialized topic.\n",
      "\n",
      "```python\n",
      "from typing import Any, List\n",
      "from InstructorEmbedding import INSTRUCTOR\n",
      "from llama_index.embeddings.base import BaseEmbedding\n",
      "\n",
      "class InstructorEmbeddings(BaseEmbedding):\n",
      "  def __init__(\n",
      "    self, \n",
      "    instructor_model_name: str = \"hkunlp/instructor-large\",\n",
      "    instruction: str = \"Represent the Computer Science documentation or question:\",\n",
      "    **kwargs: Any,\n",
      "  ) -> None:\n",
      "    self._model = INSTRUCTOR(instructor_model_name)\n",
      "    self._instruction = instruction\n",
      "    super().__init__(**kwargs)\n",
      "\n",
      "...\n",
      "\n",
      "> Source (Doc id: febe4227-a003-4d29-b1c7-16527541b8ac): Defining a custom prompt\n",
      "\n",
      "Defining a custom prompt is as simple as creating a format string\n",
      "\n",
      "```python\n",
      "from llama_index import Prompt\n",
      "\n",
      "template = (\n",
      "    \"We have provided context information below. \\n\"\n",
      "    \"---------------------\\n\"\n",
      "    \"{context_str}\"\n",
      "    \"\\n---------------------\\n\"\n",
      "    \"Given this information, please answer the question: {query_str}\\n\"\n",
      ")\n",
      "qa_template = Prompt(template)\n",
      "```\n",
      "\n",
      "> Note: you may see references to legacy prompt subclasses such as `QuestionAnswerPrompt`, `RefinePrompt`. These have been deprecated (and now are type aliases of `Prompt`). Now you can directly specify `Prompt(template)` to construct custom prompts. But you still have to make sure the template string contains the expected parameters (e.g. `{context_str}` and `{query_str}`) when replacing a default question answer prompt.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What is the purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation?\"\n",
    ")\n",
    "print(str(response))\n",
    "print(\"-----------------\")\n",
    "print(response.get_formatted_sources(length=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the `RouterQueryEngine` in LlamaIndex is to direct queries to the suitable query transformations and execute them in a sequential order. It is used in the search pipeline to process queries and guide them to the appropriate query transformations and modules within the pipeline. The `RouterQueryEngine` manages the query flow through the pipeline, ensuring that query transformations are applied before the queries are executed against the index structures.\n",
      "-----------------\n",
      "> Source (Doc id: 95065ce7-4443-4c13-bdaa-ecbd66553223): Sub question: What is the role of the RouterQueryEngine in LlamaIndex?\n",
      "Response: The RouterQueryEngine in LlamaIndex plays a crucial role in directing queries to the appropriate query transformations and executing them in a sequential manner.\n",
      "\n",
      "> Source (Doc id: 7f16ff62-4595-432a-97cc-5670946731ee): Sub question: How does the RouterQueryEngine interact with the search pipeline in LlamaIndex?\n",
      "Response: The RouterQueryEngine interacts with the search pipeline in LlamaIndex by processing queries and directing them to the appropriate query transformations and modules within the pipeline. It manages the flow of queries through the pipeline, ensuring that query transformations are applied as needed before the queries are executed against the index structures.\n",
      "\n",
      "> Source (Doc id: a9428134-90d5-4e9e-b6d2-cf74a36626e9): Query Transformations\n",
      "\n",
      "\n",
      "LlamaIndex allows you to perform *query transformations* over your index structures.\n",
      "Query transformations are modules that will convert a query into another query. They can be **single-step**, as in the transformation is run once before the query is executed against an index. \n",
      "\n",
      "They can also be **multi-step**, as in: \n",
      "1. The query is transformed, executed against an index, \n",
      "2. The response is retrieved.\n",
      "3. Subsequent queries are transformed/executed in a sequential fashion.\n",
      "\n",
      "We list some of our query transformations in more detail below.\n",
      "\n",
      "> Source (Doc id: fb688a94-7ba9-4834-9b28-b50e1b18be3b): Concept\n",
      "Query engine is a generic interface that allows you to ask question over your data.\n",
      "\n",
      "A query engine takes in a natural language query, and returns a rich response.\n",
      "It is most often (but not always) built on one or many Indices via Retrievers.\n",
      "You can compose multiple query engines to achieve more advanced capability.\n",
      "\n",
      "```{tip}\n",
      "If you want to have a conversation with your data (multiple back-and-forth instead of a single question & answer), take a look at Chat Engine  \n",
      "```\n",
      "\n",
      "> Source (Doc id: a9428134-90d5-4e9e-b6d2-cf74a36626e9): Query Transformations\n",
      "\n",
      "\n",
      "LlamaIndex allows you to perform *query transformations* over your index structures.\n",
      "Query transformations are modules that will convert a query into another query. They can be **single-step**, as in the transformation is run once before the query is executed against an index. \n",
      "\n",
      "They can also be **multi-step**, as in: \n",
      "1. The query is transformed, executed against an index, \n",
      "2. The response is retrieved.\n",
      "3. Subsequent queries are transformed/executed in a sequential fashion.\n",
      "\n",
      "We list some of our query transformations in more detail below.\n",
      "\n",
      "> Source (Doc id: a2c07582-991c-4105-9632-c33341a278e8): Streaming\n",
      "\n",
      "LlamaIndex supports streaming the response as it's being generated.\n",
      "This allows you to start printing or processing the beginning of the response before the full response is finished.\n",
      "This can drastically reduce the perceived latency of queries.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What is the purpose of the `RouterQueryEngine` in LlamaIndex and how can it be used in the search pipeline?\"\n",
    ")\n",
    "print(str(response))\n",
    "print(\"-----------------\")\n",
    "print(response.get_formatted_sources(length=1000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Response for Answer Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from llama_index.core import Response\n",
    "\n",
    "\n",
    "def evaluate_query_engine(evaluator, query_engine, questions):\n",
    "    async def run_query(query_engine, q):\n",
    "        try:\n",
    "            return await query_engine.aquery(q)\n",
    "        except:\n",
    "            return Response(response=\"Error, query failed.\")\n",
    "\n",
    "    total_correct = 0\n",
    "    all_results = []\n",
    "    for batch_size in range(0, len(questions), 5):\n",
    "        batch_qs = questions[batch_size : batch_size + 5]\n",
    "\n",
    "        tasks = [run_query(query_engine, q) for q in batch_qs]\n",
    "        responses = asyncio.run(asyncio.gather(*tasks))\n",
    "        print(f\"finished batch {(batch_size // 5) + 1} out of {len(questions) // 5}\")\n",
    "\n",
    "        for question, response in zip(batch_qs, responses):\n",
    "            eval_result = 1 if \"YES\" in evaluator.evaluate(question, response) else 0\n",
    "            total_correct += eval_result\n",
    "            all_results.append(eval_result)\n",
    "\n",
    "        # helps avoid rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    return total_correct, all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpt4_service_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QueryResponseEvaluator\n\u001b[1;32m----> 3\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m QueryResponseEvaluator(service_context\u001b[38;5;241m=\u001b[39m\u001b[43mgpt4_service_context\u001b[49m)\n\u001b[0;32m      5\u001b[0m total_correct, all_results \u001b[38;5;241m=\u001b[39m evaluate_query_engine(\n\u001b[0;32m      6\u001b[0m     evaluator, query_engine, question_dataset\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse satisfies the query? Scored \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_correct\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(question_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m questions correctly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gpt4_service_context' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import RelevancyEvaluator\n",
    "\n",
    "evaluator = RelevancyEvaluator()\n",
    "\n",
    "total_correct, all_results = evaluate_query_engine(\n",
    "    evaluator, query_engine, question_dataset\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Response satisfies the query? Scored {total_correct} out of {len(question_dataset)} questions correctly.\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating Incorrect Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the SubQuestionQueryEngine class in LlamaIndex?'\n",
      " 'What are the available options for the storage backend of the index store in LlamaIndex?'\n",
      " 'What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?'\n",
      " \"What is the purpose of the `CollectionQueryConsumer` class in the Delphic application's WebSocket handling?\"\n",
      " 'What is the purpose of the `ReActAgent` and how can it be initialized with other agents as tools?'\n",
      " 'How can I create a Django superuser using the Delphic application?'\n",
      " 'What is the default number of LLM calls required for the ListIndex?'\n",
      " 'What are the different vector stores supported by LlamaIndex for use as the storage backend for `VectorStoreIndex`?'\n",
      " 'What is the purpose of the \"router query engine\" in the LlamaIndex framework?'\n",
      " 'What storage backends are supported by LlamaIndex for persisting data?'\n",
      " 'What is the purpose of the `fetchDocuments` function in the `fetchDocuments.tsx` file in the React frontend?'\n",
      " 'What is the purpose of the `RefinePrompt` class in the LlamaIndex documentation?'\n",
      " \"What is the function used to retrieve the collections for the logged-in user in the Delphic project's frontend?\"\n",
      " 'What is the purpose of the ResponseEvaluator class in the LlamaIndex library?'\n",
      " 'What is the purpose of the Algovera tool built on top of LlamaIndex?'\n",
      " 'What is the purpose of the HyDE query transform in the LlamaIndex?'\n",
      " 'What are the three primary sections within the layout of the ChatView component?'\n",
      " 'What is the purpose of the `insert_terms` function in the LlamaIndex app?'\n",
      " 'What is the purpose of the `load_collection_model` function in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the SQLTableNodeMapping object in the LlamaIndex documentation sample?']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unanswered_queries = np.array(question_dataset)[np.array(all_results) == 0]\n",
    "print(unanswered_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the `ReActAgent` is to instantiate an agent from a set of Tools. It can be initialized with other agents as tools by passing them as parameters to the `from_tools()` method.\n",
      "-----------------\n",
      "> Source (Doc id: 809486e9-ee91-42cb-a620-66c534c6890f): Sub question: What is the purpose of the ReActAgent?\n",
      "Response: The purpose of the ReActAgent is to instantiate an agent from a set of Tools....\n",
      "\n",
      "> Source (Doc id: 581d674e-6550-4a40-aa78-8936b53867e8): Sub question: How can the ReActAgent be initialized with other agents as tools?\n",
      "Response: Based on the given context information, it is not possible to initialize the ReActAgent with other agents as tools. The ReActAgent can only be initialized with a s...\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What is the purpose of the `ReActAgent` and how can it be initialized with other agents as tools?\"\n",
    ")\n",
    "print(str(response))\n",
    "print(\"-----------------\")\n",
    "print(response.get_formatted_sources(length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is to provide a high-level interface for ingesting, indexing, and querying external data. It allows users to customize storage components such as document stores, index stores, and vector stores. The LoadAndSearchToolSpec also supports persisting data to various storage backends and offers different ways of querying a list index, including embedding-based queries and keyword filters.\n",
      "-----------------\n",
      "> Source (Doc id: 8ad62670-cacc-4a34-bb9b-00504904b04c): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is not mentioned in the given context information....\n",
      "\n",
      "> Source (Doc id: f5974d32-081a-4366-9e8a-ea9b4bccbdaf): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is to provide a high-level interface for ingesting, indexing, and querying...\n",
      "\n",
      "> Source (Doc id: 69d8cee2-a4eb-4183-acbe-34047f3b2649): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is to provide a tool specification that can be used to load and search dat...\n",
      "\n",
      "> Source (Doc id: be322260-e24b-4dc3-9540-8b973b1afecf): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: Based on the given context information, it is not possible to determine the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation. T...\n",
      "\n",
      "> Source (Doc id: f0f035ac-2c2d-4ba6-b84b-77d88bb7aaed): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: Based on the given context information, there is no mention of the \"LoadAndSearchToolSpec\" in the LlamaIndex documentation. Therefore, it is not po...\n",
      "\n",
      "> Source (Doc id: b0384b51-bb28-48dd-98fd-63f57d1c4791): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: Based on the given context information, there is no mention of the \"LoadAndSearchToolSpec\" in the LlamaIndex documentation. Therefore, the purpose ...\n",
      "\n",
      "> Source (Doc id: 9d0c096e-9049-4c36-9f1a-5b166c93a635): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is not mentioned in the given context information....\n",
      "\n",
      "> Source (Doc id: 65259c15-1ad2-4211-bd11-7c7b6f182cf8): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is to provide a tool that allows users to load and search for embeddings a...\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\"\n",
    ")\n",
    "print(str(response))\n",
    "print(\"-----------------\")\n",
    "print(response.get_formatted_sources(length=256))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we covered several key topics!\n",
    "\n",
    "- setting up a sub-question query engine\n",
    "- generating a dataset of evaluation questions\n",
    "- evaluating responses for hallucination\n",
    "- evaluating responses for answer quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
